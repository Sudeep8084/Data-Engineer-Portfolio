# AWS Customer Concessions Data Pipeline

## ğŸ“Œ Overview
This project demonstrates my contribution to a data pipeline at Amazon that ingests, processes, and aggregates customer concession data from internal tools. The data is used for compliance reporting, trend analysis, and stakeholder dashboards.

## âš™ï¸ Technologies Used
- **Language**: Python
- **Cloud**: AWS (S3, Lambda, Redshift)
- **ETL Orchestration**: Custom Python scripts
- **Monitoring**: CloudWatch
- **Data Storage**: Redshift, Parquet files in S3
- **Visualization**: Snowflake + internal dashboards

## ğŸ”„ Data Flow
1. Extract: Pull JSON files from internal microservices via APIs.
2. Transform: Clean, flatten, and normalize the data using Python.
3. Load: Store processed data into Redshift and publish to Snowflake.
4. Monitor: Use CloudWatch to alert on pipeline failures.

## ğŸ“Š Impact
- Improved data pipeline performance by 35% by optimizing file partitioning.
- Resolved a critical production issue that restored downstream analytics with zero data loss.
- Enabled compliance dashboards to load 40% faster through query optimization.

## ğŸ§ª Testing & Validation
- Unit tested all Python transformation functions.
- Validated Redshift table schemas against business rules.
- Conducted A/B testing on data load logic to improve batch success rate.

## ğŸ“ Structure
aws_customer_concessions_pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â”œâ”€â”€ load.py
â”‚ â””â”€â”€ utils.py
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ data_quality_checks.ipynb
â”œâ”€â”€ dashboards/
â”‚ â””â”€â”€ dashboard_mockup.png
â””â”€â”€ config/
â””â”€â”€ pipeline_config.yaml


### 2. `src/` folder

- `extract.py` â€“ handles API data pulls
- `transform.py` â€“ flattens/cleans JSON
- `load.py` â€“ writes to Redshift
- `utils.py` â€“ helper functions (logging, config loading)

---

### 3. `notebooks/` folder

- Exploratory data analysis (EDA)
- Data profiling
- Quality checks
- Example queries (Snowflake, Redshift)

---

### 4. `config/` folder
- S3 bucket paths
- Database credentials (mock or env-based)
- File partitions or table schemas

---

